{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266b03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectorbt as vbt\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split# Import additional libraries for dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c95765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# First, let's create more features to make dimension reduction more meaningful\n",
    "def create_extended_features(df):\n",
    "    \"\"\"\n",
    "    Create additional technical indicators to have more features for dimension reduction.\n",
    "    This demonstrates the value of dimension reduction when dealing with many features.\n",
    "    \"\"\"\n",
    "    # Momentum indicators\n",
    "    df['RSI_21'] = ta.rsi(df.Close, length=21)\n",
    "    df['Williams_R'] = ta.willr(df.High, df.Low, df.Close, length=14)\n",
    "    df['ROC'] = ta.roc(df.Close, length=10)\n",
    "    \n",
    "    # Moving averages with different periods\n",
    "    df['SMA_10'] = ta.sma(df.Close, length=10)\n",
    "    df['SMA_20'] = ta.sma(df.Close, length=20)\n",
    "    df['EMA_12'] = ta.ema(df.Close, length=12)\n",
    "    df['EMA_26'] = ta.ema(df.Close, length=26)\n",
    "    \n",
    "    # Volatility indicators\n",
    "    df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.bbands(df.Close, length=20).iloc[:, 0], ta.bbands(df.Close, length=20).iloc[:, 1], ta.bbands(df.Close, length=20).iloc[:, 2]\n",
    "    df['BB_width'] = df['BBANDS_upper'] - df['BBANDS_lower']\n",
    "    df['BB_position'] = (df['Close'] - df['BBANDS_lower']) / (df['BBANDS_upper'] - df['BBANDS_lower'])\n",
    "    \n",
    "    # Volume indicators (if volume data was available)\n",
    "    # df['Volume_SMA'] = ta.sma(df.Volume, length=20)\n",
    "    # df['Volume_ratio'] = df.Volume / df['Volume_SMA']\n",
    "    \n",
    "    # Price-based features\n",
    "    df['High_Low_ratio'] = df['High'] / df['Low']\n",
    "    df['Close_Open_ratio'] = df['Close'] / df['Open']\n",
    "    df['Price_change'] = df['Close'].pct_change()\n",
    "    df['Price_change_3d'] = df['Close'].pct_change(periods=3)\n",
    "    \n",
    "    # Lag features (previous day values)\n",
    "    df['Close_lag1'] = df['Close'].shift(1)\n",
    "    df['Close_lag2'] = df['Close'].shift(2)\n",
    "    df['RSI_lag1'] = df['RSI'].shift(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply extended feature creation\n",
    "df = create_extended_features(df)\n",
    "\n",
    "# Define extended feature list\n",
    "extended_features = [\n",
    "    'RSI', 'SMA', 'EMA', 'ATR', 'MACD', 'MACD_signal',\n",
    "    'RSI_21', 'Williams_R', 'ROC', 'SMA_10', 'SMA_20', \n",
    "    'EMA_12', 'EMA_26', 'BBANDS_upper', 'BBANDS_middle', \n",
    "    'BBANDS_lower', 'BB_width', 'BB_position', 'High_Low_ratio',\n",
    "    'Close_Open_ratio', 'Price_change', 'Price_change_3d',\n",
    "    'Close_lag1', 'Close_lag2', 'RSI_lag1'\n",
    "]\n",
    "\n",
    "print(f\"Total features created: {len(extended_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edba8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data and prepare for dimension reduction\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_clean.dropna(inplace=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X_extended = df_clean[extended_features]\n",
    "y_extended = df_clean['Signal']\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {X_extended.shape}\")\n",
    "print(f\"Features: {X_extended.columns.tolist()}\")\n",
    "\n",
    "# Check for any remaining NaN values\n",
    "print(f\"NaN values in features: {X_extended.isnull().sum().sum()}\")\n",
    "print(f\"NaN values in target: {y_extended.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2da150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSION REDUCTION TECHNIQUE 1: CORRELATION-BASED FEATURE REMOVAL\n",
    "def remove_highly_correlated_features(X, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Remove features that are highly correlated with other features.\n",
    "    High correlation indicates redundancy in features.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        threshold: Correlation threshold above which features are considered redundant\n",
    "    \n",
    "    Returns:\n",
    "        X_reduced: Feature matrix with highly correlated features removed\n",
    "        removed_features: List of removed feature names\n",
    "    \"\"\"\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "    \n",
    "    # Create a mask for the upper triangle (to avoid duplicate comparisons)\n",
    "    upper_triangle = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "    \n",
    "    # Find features to remove (correlation > threshold)\n",
    "    to_remove = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if corr_matrix.iloc[i, j] > threshold:\n",
    "                # Remove the feature with lower mean correlation to all other features\n",
    "                mean_corr_i = corr_matrix.iloc[i, :].mean()\n",
    "                mean_corr_j = corr_matrix.iloc[j, :].mean()\n",
    "                if mean_corr_i > mean_corr_j:\n",
    "                    to_remove.append(corr_matrix.columns[j])\n",
    "                else:\n",
    "                    to_remove.append(corr_matrix.columns[i])\n",
    "    \n",
    "    # Remove duplicates and create reduced feature set\n",
    "    to_remove = list(set(to_remove))\n",
    "    X_reduced = X.drop(columns=to_remove)\n",
    "    \n",
    "    print(f\"Removed {len(to_remove)} highly correlated features: {to_remove}\")\n",
    "    print(f\"Remaining features: {X_reduced.shape[1]}\")\n",
    "    \n",
    "    return X_reduced, to_remove\n",
    "\n",
    "# Apply correlation-based feature removal\n",
    "X_corr_reduced, removed_corr_features = remove_highly_correlated_features(X_extended, threshold=0.9)\n",
    "\n",
    "# Visualize correlation matrix before and after\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(X_extended.corr(), annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Original Feature Correlations')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(X_corr_reduced.corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('After Correlation-based Reduction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSION REDUCTION TECHNIQUE 2: PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
    "def apply_pca_reduction(X, n_components=None, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Apply PCA for dimension reduction while preserving specified variance.\n",
    "    \n",
    "    PCA finds the principal components (linear combinations of original features)\n",
    "    that capture the most variance in the data. This helps reduce noise and\n",
    "    computational complexity while retaining most of the information.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        n_components: Number of components to keep (if None, use variance_threshold)\n",
    "        variance_threshold: Minimum cumulative variance to preserve\n",
    "    \n",
    "    Returns:\n",
    "        X_pca: Transformed feature matrix\n",
    "        pca: Fitted PCA object\n",
    "        n_components_used: Number of components actually used\n",
    "    \"\"\"\n",
    "    # Standardize features (important for PCA)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    if n_components is None:\n",
    "        # Determine number of components needed for variance threshold\n",
    "        pca_temp = PCA()\n",
    "        pca_temp.fit(X_scaled)\n",
    "        cumsum_variance = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum_variance >= variance_threshold) + 1\n",
    "    \n",
    "    # Apply PCA with determined number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Print information about the reduction\n",
    "    total_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"PCA Reduction Results:\")\n",
    "    print(f\"Original dimensions: {X.shape[1]}\")\n",
    "    print(f\"Reduced dimensions: {n_components}\")\n",
    "    print(f\"Variance preserved: {total_variance:.3f} ({total_variance*100:.1f}%)\")\n",
    "    print(f\"Compression ratio: {X.shape[1]/n_components:.2f}:1\")\n",
    "    \n",
    "    # Create DataFrame with PCA components\n",
    "    pca_columns = [f'PC{i+1}' for i in range(n_components)]\n",
    "    X_pca_df = pd.DataFrame(X_pca, index=X.index, columns=pca_columns)\n",
    "    \n",
    "    return X_pca_df, pca, scaler, n_components\n",
    "\n",
    "# Apply PCA to correlation-reduced features\n",
    "X_pca, pca_model, pca_scaler, n_pca_components = apply_pca_reduction(\n",
    "    X_corr_reduced, \n",
    "    variance_threshold=0.95\n",
    ")\n",
    "\n",
    "# Visualize PCA results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Explained variance ratio\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(range(1, len(pca_model.explained_variance_ratio_) + 1), \n",
    "        pca_model.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Variance Explained by Each PC')\n",
    "\n",
    "# Plot 2: Cumulative explained variance\n",
    "plt.subplot(1, 3, 2)\n",
    "cumsum_variance = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'bo-')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Variance Explained')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Feature contributions to first two PCs\n",
    "plt.subplot(1, 3, 3)\n",
    "feature_contributions = pca_model.components_[:2].T\n",
    "plt.scatter(feature_contributions[:, 0], feature_contributions[:, 1])\n",
    "plt.xlabel('PC1 Contribution')\n",
    "plt.ylabel('PC2 Contribution')\n",
    "plt.title('Feature Contributions to PC1 vs PC2')\n",
    "\n",
    "# Add feature labels\n",
    "for i, feature in enumerate(X_corr_reduced.columns):\n",
    "    plt.annotate(feature, (feature_contributions[i, 0], feature_contributions[i, 1]), \n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSION REDUCTION TECHNIQUE 3: UNIVARIATE FEATURE SELECTION\n",
    "def apply_univariate_selection(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Select k best features based on univariate statistical tests.\n",
    "    \n",
    "    This method evaluates each feature individually against the target variable\n",
    "    using statistical tests (F-test for classification). Features with the\n",
    "    highest scores are selected.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target variable\n",
    "        k: Number of features to select\n",
    "    \n",
    "    Returns:\n",
    "        X_selected: Feature matrix with selected features\n",
    "        selector: Fitted SelectKBest object\n",
    "        selected_features: Names of selected features\n",
    "    \"\"\"\n",
    "    # Apply SelectKBest with f_classif scoring function\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    # Get feature scores\n",
    "    feature_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': selector.scores_,\n",
    "        'Selected': selector.get_support()\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    \n",
    "    print(f\"Univariate Feature Selection Results:\")\n",
    "    print(f\"Selected {k} features out of {X.shape[1]}\")\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    print(\"\\nTop 10 feature scores:\")\n",
    "    print(feature_scores.head(10))\n",
    "    \n",
    "    # Create DataFrame with selected features\n",
    "    X_selected_df = pd.DataFrame(X_selected, index=X.index, columns=selected_features)\n",
    "    \n",
    "    return X_selected_df, selector, selected_features, feature_scores\n",
    "\n",
    "# Apply univariate feature selection\n",
    "X_univariate, univariate_selector, selected_features, feature_scores = apply_univariate_selection(\n",
    "    X_corr_reduced, y_extended, k=8\n",
    ")\n",
    "\n",
    "# Visualize feature selection results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(range(len(feature_scores)), feature_scores['Score'])\n",
    "plt.yticks(range(len(feature_scores)), feature_scores['Feature'])\n",
    "plt.xlabel('F-Score')\n",
    "plt.title('Feature Importance Scores')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "selected_scores = feature_scores[feature_scores['Selected']]['Score']\n",
    "plt.bar(range(len(selected_scores)), selected_scores)\n",
    "plt.xticks(range(len(selected_scores)), \n",
    "           feature_scores[feature_scores['Selected']]['Feature'], \n",
    "           rotation=45, ha='right')\n",
    "plt.ylabel('F-Score')\n",
    "plt.title('Selected Features and Their Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cebce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSION REDUCTION TECHNIQUE 4: RECURSIVE FEATURE ELIMINATION (RFE)\n",
    "def apply_rfe_selection(X, y, n_features=8, estimator=None):\n",
    "    \"\"\"\n",
    "    Apply Recursive Feature Elimination using XGBoost as the estimator.\n",
    "    \n",
    "    RFE works by recursively eliminating features and building the model\n",
    "    on the remaining attributes. It uses the model's feature importance\n",
    "    to eliminate the least important features.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target variable\n",
    "        n_features: Number of features to select\n",
    "        estimator: Base estimator (if None, uses XGBoost)\n",
    "    \n",
    "    Returns:\n",
    "        X_rfe: Feature matrix with selected features\n",
    "        rfe: Fitted RFE object\n",
    "        selected_features: Names of selected features\n",
    "    \"\"\"\n",
    "    if estimator is None:\n",
    "        # Use XGBoost as the base estimator for RFE\n",
    "        estimator = xgb.XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "    \n",
    "    # Map target values for XGBoost classifier\n",
    "    y_mapped = y.map({-1: 0, 0: 1, 1: 2})\n",
    "    \n",
    "    # Apply RFE\n",
    "    rfe = RFE(estimator=estimator, n_features_to_select=n_features, step=1)\n",
    "    X_rfe = rfe.fit_transform(X, y_mapped)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[rfe.support_].tolist()\n",
    "    \n",
    "    # Get feature rankings\n",
    "    feature_rankings = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Ranking': rfe.ranking_,\n",
    "        'Selected': rfe.support_\n",
    "    }).sort_values('Ranking')\n",
    "    \n",
    "    print(f\"RFE Results:\")\n",
    "    print(f\"Selected {n_features} features out of {X.shape[1]}\")\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    print(\"\\nFeature rankings:\")\n",
    "    print(feature_rankings)\n",
    "    \n",
    "    # Create DataFrame with selected features\n",
    "    X_rfe_df = pd.DataFrame(X_rfe, index=X.index, columns=selected_features)\n",
    "    \n",
    "    return X_rfe_df, rfe, selected_features, feature_rankings\n",
    "\n",
    "# Apply RFE selection\n",
    "X_rfe, rfe_selector, rfe_selected_features, feature_rankings = apply_rfe_selection(\n",
    "    X_corr_reduced, y_extended, n_features=6\n",
    ")\n",
    "\n",
    "# Visualize RFE results\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if selected else 'red' for selected in feature_rankings['Selected']]\n",
    "plt.barh(range(len(feature_rankings)), feature_rankings['Ranking'], color=colors)\n",
    "plt.yticks(range(len(feature_rankings)), feature_rankings['Feature'])\n",
    "plt.xlabel('Ranking (1 = best)')\n",
    "plt.title('RFE Feature Rankings (Green = Selected, Red = Eliminated)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c80a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON OF DIMENSION REDUCTION TECHNIQUES\n",
    "def compare_dimension_reduction_methods(X_sets, y, method_names):\n",
    "    \"\"\"\n",
    "    Compare different dimension reduction methods by training XGBoost models\n",
    "    and evaluating their performance.\n",
    "    \n",
    "    Args:\n",
    "        X_sets: List of feature matrices from different reduction methods\n",
    "        y: Target variable\n",
    "        method_names: Names of the reduction methods\n",
    "    \n",
    "    Returns:\n",
    "        results: DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, (X, method_name) in enumerate(zip(X_sets, method_names)):\n",
    "        print(f\"\\nEvaluating {method_name}...\")\n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        \n",
    "        # Split data (time-based split)\n",
    "        X_sorted = X.sort_index()\n",
    "        y_sorted = y.sort_index()\n",
    "        \n",
    "        split_point = int(len(X_sorted) * 0.8)\n",
    "        X_train = X_sorted.iloc[:split_point]\n",
    "        X_test = X_sorted.iloc[split_point:]\n",
    "        y_train = y_sorted.iloc[:split_point]\n",
    "        y_test = y_sorted.iloc[split_point:]\n",
    "        \n",
    "        # Map target values\n",
    "        y_train_mapped = y_train.map({-1: 0, 0: 1, 1: 2})\n",
    "        y_test_mapped = y_test.map({-1: 0, 0: 1, 1: 2})\n",
    "        \n",
    "        # Train XGBoost model\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train_mapped)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test_mapped)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'eta': 0.1,\n",
    "            'max_depth': 6,\n",
    "            'seed': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Train model with early stopping\n",
    "        evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "        bst = xgb.train(\n",
    "            params, dtrain, \n",
    "            num_boost_round=100,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=10,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = bst.predict(dtest)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(y_pred == y_test_mapped)\n",
    "        \n",
    "        # Get best iteration and test error\n",
    "        best_iteration = bst.best_iteration\n",
    "        test_error = bst.best_score\n",
    "        \n",
    "        results.append({\n",
    "            'Method': method_name,\n",
    "            'Features': X.shape[1],\n",
    "            'Accuracy': accuracy,\n",
    "            'Test_Error': test_error,\n",
    "            'Best_Iteration': best_iteration,\n",
    "            'Feature_Names': list(X.columns) if hasattr(X, 'columns') else None\n",
    "        })\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test Error: {test_error:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Prepare datasets for comparison\n",
    "comparison_datasets = [\n",
    "    X_extended,  # Original features\n",
    "    X_corr_reduced,  # Correlation-based reduction\n",
    "    X_pca,  # PCA reduction\n",
    "    X_univariate,  # Univariate selection\n",
    "    X_rfe  # RFE selection\n",
    "]\n",
    "\n",
    "method_names = [\n",
    "    'Original Features',\n",
    "    'Correlation Reduction',\n",
    "    'PCA',\n",
    "    'Univariate Selection',\n",
    "    'RFE Selection'\n",
    "]\n",
    "\n",
    "# Compare methods\n",
    "comparison_results = compare_dimension_reduction_methods(\n",
    "    comparison_datasets, y_extended, method_names\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DIMENSION REDUCTION COMPARISON RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(comparison_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE COMPARISON RESULTS\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.bar(comparison_results['Method'], comparison_results['Accuracy'])\n",
    "plt.title('Model Accuracy by Reduction Method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 2: Number of features\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.bar(comparison_results['Method'], comparison_results['Features'])\n",
    "plt.title('Number of Features by Method')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 3: Test error\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.bar(comparison_results['Method'], comparison_results['Test_Error'])\n",
    "plt.title('Test Error by Method')\n",
    "plt.ylabel('Test Error (Log Loss)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: Accuracy vs Number of Features\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(comparison_results['Features'], comparison_results['Accuracy'])\n",
    "for i, method in enumerate(comparison_results['Method']):\n",
    "    plt.annotate(method, \n",
    "                (comparison_results['Features'].iloc[i], \n",
    "                 comparison_results['Accuracy'].iloc[i]),\n",
    "                fontsize=8, alpha=0.7)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Feature Count')\n",
    "\n",
    "# Plot 5: Training efficiency (Best iteration)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar(comparison_results['Method'], comparison_results['Best_Iteration'])\n",
    "plt.title('Training Iterations to Best Score')\n",
    "plt.ylabel('Best Iteration')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 6: Efficiency ratio (Accuracy per Feature)\n",
    "plt.subplot(2, 3, 6)\n",
    "efficiency_ratio = comparison_results['Accuracy'] / comparison_results['Features']\n",
    "plt.bar(comparison_results['Method'], efficiency_ratio)\n",
    "plt.title('Efficiency Ratio (Accuracy/Features)')\n",
    "plt.ylabel('Efficiency Ratio')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the best performing method\n",
    "best_method_idx = comparison_results['Accuracy'].idxmax()\n",
    "best_method = comparison_results.iloc[best_method_idx]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"BEST PERFORMING METHOD\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Method: {best_method['Method']}\")\n",
    "print(f\"Features: {best_method['Features']}\")\n",
    "print(f\"Accuracy: {best_method['Accuracy']:.4f}\")\n",
    "print(f\"Test Error: {best_method['Test_Error']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT FINAL MODEL WITH BEST DIMENSION REDUCTION METHOD\n",
    "def implement_final_model_with_reduction():\n",
    "    \"\"\"\n",
    "    Implement the final trading model using the best dimension reduction method.\n",
    "    This function shows how to integrate dimension reduction into your trading pipeline.\n",
    "    \"\"\"\n",
    "    # Based on comparison results, select the best method\n",
    "    # For this example, let's assume RFE performed best\n",
    "    print(\"Implementing final model with RFE-selected features...\")\n",
    "    \n",
    "    # Use RFE-selected features\n",
    "    X_final = X_rfe.copy()\n",
    "    y_final = y_extended.copy()\n",
    "    \n",
    "    # Time-based split for final evaluation\n",
    "    split_point = int(len(X_final) * 0.8)\n",
    "    X_train_final = X_final.iloc[:split_point]\n",
    "    X_test_final = X_final.iloc[split_point:]\n",
    "    y_train_final = y_final.iloc[:split_point]\n",
    "    y_test_final = y_final.iloc[split_point:]\n",
    "    \n",
    "    # Map target values\n",
    "    y_train_mapped = y_train_final.map({-1: 0, 0: 1, 1: 2})\n",
    "    y_test_mapped = y_test_final.map({-1: 0, 0: 1, 1: 2})\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain_final = xgb.DMatrix(X_train_final, label=y_train_mapped)\n",
    "    dtest_final = xgb.DMatrix(X_test_final, label=y_test_mapped)\n",
    "    \n",
    "    # Optimized XGBoost parameters for the reduced feature set\n",
    "    params_final = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'eta': 0.1,\n",
    "        'max_depth': 4,  # Reduced depth due to fewer features\n",
    "        'min_child_weight': 3,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Train final model\n",
    "    evals = [(dtrain_final, 'train'), (dtest_final, 'test')]\n",
    "    final_model = xgb.train(\n",
    "        params_final, dtrain_final,\n",
    "        num_boost_round=200,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_final = final_model.predict(dtest_final)\n",
    "    \n",
    "    # Map predictions back to trading signals\n",
    "    y_pred_signals = pd.Series(y_pred_final, index=y_test_final.index).map({0: -1, 1: 0, 2: 1})\n",
    "    \n",
    "    # Create full signal series for backtesting\n",
    "    full_signals = pd.Series(0, index=df_clean.index)\n",
    "    full_signals.update(y_pred_signals)\n",
    "    \n",
    "    return final_model, full_signals, X_final.columns.tolist()\n",
    "\n",
    "# Implement final model\n",
    "final_model, trading_signals, final_features = implement_final_model_with_reduction()\n",
    "\n",
    "print(f\"\\nFinal model trained with {len(final_features)} features:\")\n",
    "print(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKTEST THE FINAL MODEL WITH DIMENSION REDUCTION\n",
    "def backtest_reduced_model(df, signals):\n",
    "    \"\"\"\n",
    "    Backtest the trading strategy using the dimension-reduced model.\n",
    "    \n",
    "    Args:\n",
    "        df: Price data DataFrame\n",
    "        signals: Trading signals from the reduced model\n",
    "    \n",
    "    Returns:\n",
    "        portfolio: VectorBT portfolio object\n",
    "    \"\"\"\n",
    "    # Create entry and exit signals\n",
    "    entry_signals = signals == 1\n",
    "    exit_signals = signals == -1\n",
    "    \n",
    "    # Backtest with vectorbt\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['Close'],\n",
    "        entries=entry_signals,\n",
    "        exits=exit_signals,\n",
    "        freq='D',\n",
    "        init_cash=10000,\n",
    "        fees=0.001,\n",
    "        sl_stop=0.05,  # 5% stop loss\n",
    "        upon_opposite_entry='ignore'\n",
    "    )\n",
    "    \n",
    "    return portfolio\n",
    "\n",
    "# Backtest the final model\n",
    "portfolio_reduced = backtest_reduced_model(df_clean, trading_signals)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BACKTESTING RESULTS - DIMENSION REDUCED MODEL\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Return: {portfolio_reduced.total_return():.2%}\")\n",
    "print(f\"Annualized Return: {portfolio_reduced.annualized_return():.2%}\")\n",
    "print(f\"Max Drawdown: {portfolio_reduced.max_drawdown():.2%}\")\n",
    "print(f\"Sharpe Ratio: {portfolio_reduced.sharpe_ratio():.3f}\")\n",
    "print(f\"Number of Trades: {portfolio_reduced.trades.count()}\")\n",
    "print(f\"Win Rate: {portfolio_reduced.trades.win_rate():.2%}\")\n",
    "\n",
    "# Plot portfolio performance\n",
    "portfolio_reduced.plot().show()\n",
    "plt.title(\"Portfolio Performance - Dimension Reduced Model\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance from the final model\n",
    "feature_importance = final_model.get_score(importance_type='weight')\n",
    "importance_df = pd.DataFrame(\n",
    "    list(feature_importance.items()), \n",
    "    columns=['Feature', 'Importance']\n",
    ").sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance_df)), importance_df['Importance'])\n",
    "plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "plt.xlabel('Feature Importance (Weight)')\n",
    "plt.title('Final Model Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Model Feature Importance:\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
